1)Configurations to set up While Creating AWS EC2 Instance


-->1.Create t2.2xlarge EC2 instance with Amazon Linux 2 AMI (HVM)
     a. Storage- 96 GB
     b. Allow ports- 0-10000
     
     Choose t2.large instance type
     Create new key value pair
     Launch instance
     
2)Connect to the instance using SSH

    CD to folder where .pem file resides

-->ssh -i "D:\path\to\private\key.pem" user@Public_DNS

    Example: ssh -i "datapipeline.pem" ec2-user@ec2-34-235-133-172.compute-1.amazonaws.com

3) Install and setup Docker and Docker-compose

-->sudo yum update -y

-->sudo yum install docker

-->sudo curl -L "https://github.com/docker/compose/releases/download/1.29.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose

-->sudo chmod +x /usr/local/bin/docker-compose

-->sudo gpasswd -a $USER docker

-->newgrp docker

-->Start/Stop Docker
    To Start Docker - sudo systemctl start docker
    To Stop Docker - sudo systemctl stop docker
    
-->Check if docker started ,using command > docker ps

4)Copy folder from local to ec2 and give required permissions

    CD to folder where .pem file and Docker_exp folder resides

-->scp -r -i "datapipelineineuron.pem" docker_exp ec2-user@ec2-65-0-80-189.ap-south-1.compute.amazonaws.com:/home/ec2-user/docker_exp

-->sudo chmod -R 755 docker_exp

    Change directory to docker_exp

-->cd docker_exp

    List files with permissions
-->ls -l
Run shell script airflow_init.sh after starting the docker

--> ./airflow_init.sh

    3 new folders named ‘dags’, ‘logs’ and ‘plugins’ will be created

    move airflow DAG scripts to dags folder

-->cd docker_exp (run if not in the right directory)
-->mv -t dags hive_script.py hive_connection.py

    Start/Stop docker containers
    o	cd docker_exp (run if not in the right directory)
    o	docker-compose up
    o	docker-compose stop
    o	docker-compose down (stop and remove containers)
    
5)Port Forwarding to access services locally

ssh -i "datapipeline.pem" ec2-user@ec2-43-205-195-57.ap-south-1.compute.amazonaws.com -o "ServerAliveInterval 30" -L 2081:localhost:2041 -L 4888:localhost:4888 -L 4889:localhost:4889 -L 2080:localhost:2080 -L 8050:localhost:8050 -L 8051:localhost:8051 -L 4141:localhost:4141 -L 4090:localhost:4090 -L 3180:localhost:3180 -L 50075:localhost:50075 -L 50070:localhost:50070 -L 50010:localhost:50010 -L 3077:localhost:3077 -L 4080:localhost:4080 -L 9870:localhost:9870 -L 8188:localhost:8188 -L 9864:localhost:9864 -L 8042:localhost:8042 -L 8088:localhost:8088 -L 8080:localhost:8080 -L 8081:localhost:8081 -L 10000:localhost:10000 -L 6080:localhost:6080 -L 8998:localhost:8998

    Now we can access services locally
--> docker ps
    we can see all images in docker now
    
    In browser ope,
    
    nifi
-->http://localhost:2080/nifi/

    airflow
-->http://localhost:6080/
    u:airflow
    p:airflow
    
    spark master
-->http://localhost:8080/

    jupyter lab
-->localhost:4888/lab

6) Get into bash shell of different containers

    namenode
-->docker exec -i -t hdp_namenode bash

    spark master
-->docker exec -i -t hdp_spark-master bash

    hive server
-->docker exec -i -t hdp_hive-server bash

    use exit to come out of bash shell / ctrl+d
    
    
    
    





